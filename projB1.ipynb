{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"projB1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project B1: Spam/Ham Classification\n",
    "\n",
    "## Due Date: Thursday, April 24th at 11:59 PM\n",
    "\n",
    "You must submit this assignment to Gradescope by the on-time deadline, Thursday, April 24th, 11:59 PM. Please read the syllabus for the Slip Day policy. No late submissions beyond what is outlined in the Slip Day policy will be accepted. We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline. This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance (TAs need to sleep, after all!).\n",
    "\n",
    "Please read the instructions carefully when you are submitting your work to Gradescope.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the project, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the collaborators cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "proj2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "You will use what you've learned in class to create a binary classifier that can distinguish spam (junk, commercial, or bulk) emails from ham (regular non-spam) emails. In addition to providing some skeleton code to fill in, we will evaluate your work based on your model's accuracy and your written responses in this notebook.\n",
    "\n",
    "After this project, you should feel comfortable with the following:\n",
    "\n",
    "- Feature engineering with text data.\n",
    "- Using the `sklearn` library to process data and fit models.\n",
    "- Validating the performance of your model and minimizing overfitting.\n",
    "\n",
    "This first part of the project focuses on initial analysis, feature engineering, and logistic regression. In the second part of this project (which will be released next week), you will build your own spam/ham classifier.\n",
    "\n",
    "## Content Warning\n",
    "This is a **real-world** dataset â€”â€”Â the emails you are trying to classify are actual spam and legitimate emails. As a result, some of the spam emails may be in poor taste or be considered inappropriate. We think the benefit of working with realistic data outweighs these inappropriate emails but wanted to provide a warning at the beginning of the project so that you are aware.\n",
    "\n",
    "If you feel uncomfortable with this topic, **please contact your TA, the instructors, or reach out via the [Spring 2025 Additional Accomodations Form](https://docs.google.com/forms/d/e/1FAIpQLSe23BU7DocByEPYt6YV00rOTn7K1AVj7Fqpw2eOgWG0Q5GtDw/viewform).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "The iwut extension is already loaded. To reload it, use:\n",
      "  %reload_ext iwut\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to suppress all FutureWarnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# More readable exceptions.\n",
    "%pip install --quiet iwut\n",
    "%load_ext iwut\n",
    "%wut on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't worry about the note stating you may need to restart the kernel to use updated packages. Feel free to ignore that.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "Grading is broken down into autograded answers and free responses. \n",
    "\n",
    "For autograded answers, the results of your code are compared to provided and/or hidden tests.\n",
    "\n",
    "For free response, readers will evaluate how well you answered the question and/or fulfilled the requirements of the question.\n",
    "\n",
    "Question | Manual | Points\n",
    "----|----|----\n",
    "1 | Yes | 2\n",
    "2 | No | 3\n",
    "3 | Yes | 3\n",
    "4 | No | 2\n",
    "5 | No | 2\n",
    "6a | No | 1\n",
    "6b | No | 1\n",
    "6c | Yes | 2\n",
    "6d | No | 2\n",
    "6e | No | 1\n",
    "6f | Yes | 1\n",
    "6g | Yes | 1\n",
    "6h | Yes | 2\n",
    "Total | 6 | 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run code, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder.** Please be sure to check your results carefully.\n",
    "\n",
    "### Debugging Guide\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, common `pandas` errors, RegEx, visualizations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:41.341673Z",
     "start_time": "2019-04-03T20:17:41.330307Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loading",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# The Data\n",
    "\n",
    "In email classification, our goal is to classify emails as spam or not spam (referred to as \"ham\") using features generated from the text in the email. The dataset is from [SpamAssassin](https://spamassassin.apache.org/old/publiccorpus/). It consists of email messages and their labels (0 for ham, 1 for spam). Your labeled training dataset contains 8,348 labeled examples, and the unlabeled test set contains 1,000 unlabeled examples.\n",
    "\n",
    "**Note:** The dataset is from 2004, so the contents of emails might be very different from those in 2024.\n",
    "\n",
    "Run the following cells to load the data into a `DataFrame`.\n",
    "\n",
    "The `train` `DataFrame` contains labeled data you will use to train your model. It has four columns:\n",
    "\n",
    "1. `id`: An identifier for the training example.\n",
    "1. `subject`: The subject of the email.\n",
    "1. `email`: The text of the email.\n",
    "1. `spam`: 1 if the email is spam, 0 if the email is ham (not spam).\n",
    "\n",
    "The `test` `DataFrame` contains 1,000 unlabeled emails. In Project B2, you will predict labels for these emails and submit your predictions to the autograder for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Loading training and test datasets\n",
    "with zipfile.ZipFile('spam_ham_data.zip') as item:\n",
    "    with item.open(\"train.csv\") as f:\n",
    "        original_training_data = pd.read_csv(f)\n",
    "    with item.open(\"test.csv\") as f:\n",
    "        test = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: A&amp;L Daily to be auctioned in bankrupt...</td>\n",
       "      <td>url: http://boingboing.net/#85534171\\n date: n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: Wired: \"Stronger ties between ISPs an...</td>\n",
       "      <td>url: http://scriptingnews.userland.com/backiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: It's just too small                  ...</td>\n",
       "      <td>&lt;html&gt;\\n &lt;head&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n &lt;font siz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: liberal defnitions\\n</td>\n",
       "      <td>depends on how much over spending vs. how much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: RE: [ILUG] Newbie seeks advice - Suse...</td>\n",
       "      <td>hehe sorry but if you hit caps lock twice the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            subject  \\\n",
       "0   0  Subject: A&L Daily to be auctioned in bankrupt...   \n",
       "1   1  Subject: Wired: \"Stronger ties between ISPs an...   \n",
       "2   2  Subject: It's just too small                  ...   \n",
       "3   3                      Subject: liberal defnitions\\n   \n",
       "4   4  Subject: RE: [ILUG] Newbie seeks advice - Suse...   \n",
       "\n",
       "                                               email  spam  \n",
       "0  url: http://boingboing.net/#85534171\\n date: n...     0  \n",
       "1  url: http://scriptingnews.userland.com/backiss...     0  \n",
       "2  <html>\\n <head>\\n </head>\\n <body>\\n <font siz...     1  \n",
       "3  depends on how much over spending vs. how much...     0  \n",
       "4  hehe sorry but if you hit caps lock twice the ...     0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the emails to lowercase as the first step of text processing.\n",
    "original_training_data['email'] = original_training_data['email'].str.lower()\n",
    "test['email'] = test['email'].str.lower()\n",
    "\n",
    "original_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34476156ed73b800",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "\n",
    "First, let's check if our data contains any missing values. We have filled in the cell below to print the number of `NaN` values in each column. If there are `NaN` values, we replace them with appropriate filler values (i.e., `NaN` values in the `subject` or `email` columns will be replaced with empty strings). Finally, we print the number of `NaN` values in each column after this modification to verify that there are no `NaN` values left.\n",
    "\n",
    "**Note:** While there are no `NaN` values in the `spam` column, we should be careful when replacing `NaN` labels. Doing so without consideration may introduce significant bias into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.203231Z",
     "start_time": "2019-04-03T20:17:42.185104Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1fb39d9b651ca1b",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before imputation:\n",
      "id         0\n",
      "subject    6\n",
      "email      0\n",
      "spam       0\n",
      "dtype: int64\n",
      "------------\n",
      "After imputation:\n",
      "id         0\n",
      "subject    0\n",
      "email      0\n",
      "spam       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Before imputation:')\n",
    "print(original_training_data.isnull().sum())\n",
    "original_training_data = original_training_data.fillna('')\n",
    "print('------------')\n",
    "print('After imputation:')\n",
    "print(original_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: Initial Analysis\n",
    "\n",
    "In the cell below, we have printed the text of the `email` field for the first ham and the first spam email in the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.247245Z",
     "start_time": "2019-04-03T20:17:42.228451Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham Email:\n",
      "url: http://boingboing.net/#85534171\n",
      " date: not supplied\n",
      " \n",
      " arts and letters daily, a wonderful and dense blog, has folded up its tent due \n",
      " to the bankruptcy of its parent company. a&l daily will be auctioned off by the \n",
      " receivers. link[1] discuss[2] (_thanks, misha!_)\n",
      " \n",
      " [1] http://www.aldaily.com/\n",
      " [2] http://www.quicktopic.com/boing/h/zlfterjnd6jf\n",
      " \n",
      " \n",
      "\n",
      "-------------------------------------------------\n",
      "Spam Email:\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      " <font size=3d\"4\"><b> a man endowed with a 7-8\" hammer is simply<br>\n",
      "  better equipped than a man with a 5-6\"hammer. <br>\n",
      " <br>would you rather have<br>more than enough to get the job done or fall =\n",
      " short. it's totally up<br>to you. our methods are guaranteed to increase y=\n",
      " our size by 1-3\"<br> <a href=3d\"http://209.163.187.47/cgi-bin/index.php?10=\n",
      " 004\">come in here and see how</a>\n",
      " </body>\n",
      " </html>\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_ham = original_training_data.loc[original_training_data['spam'] == 0, 'email'].iloc[0]\n",
    "first_spam = original_training_data.loc[original_training_data['spam'] == 1, 'email'].iloc[0]\n",
    "print(\"Ham Email:\")\n",
    "print(first_ham)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Spam Email:\")\n",
    "print(first_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1\n",
    "\n",
    "Discuss one attribute or characteristic you notice that is different between the two emails that may allow you to uniquely identify a spam email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "The spam email has a bunch of html tags, and uses a lot more promotional languauge, and has an unreadable url. The ham email has a lot more normaly spoken text and readable less suspicious urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78513403ef52a957",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Training-Validation Split\n",
    "The training data we downloaded is all the data we have available for both training models and **validating** the models that we train. We, therefore, need to split the training data into separate training and validation datasets. You will need this **validation data** to assess the performance of your classifier once you are finished training. Note that we set the seed (`random_state`) to 42. This will produce a pseudo-random sequence of random numbers that is the same for every student. **Do not modify this random seed in the following questions, as our tests depend on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.317970Z",
     "start_time": "2019-04-03T20:17:42.294532Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-873194ed3e686dfb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This creates a 90/10 train-validation split on our labeled data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(original_training_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "feat-eng",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: Feature Engineering\n",
    "\n",
    "We want to take the text of an email and predict whether the email is ham or spam. This is a **binary classification** problem, so we can use logistic regression to train a classifier. Recall that to train a logistic regression model, we need a numeric feature matrix $\\mathbb{X}$ and a vector of corresponding binary labels $Y$. Unfortunately, our data are text, not numbers. To address this, we can create numeric features derived from the email text and use those features for logistic regression.\n",
    "\n",
    "Each row of $\\mathbb{X}$ is an email. Each column of $\\mathbb{X}$ contains one feature for all the emails. We'll guide you through creating a simple feature, and you'll create more interesting ones as you try to increase the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Create a function `words_in_texts` that takes in a list of interesting words (`words`) and a `Series` of emails (`texts`). Our goal is to check if each word in `words` is contained in the emails in `texts`.\n",
    "\n",
    "The `words_in_texts` function should output a **2-dimensional `NumPy` array** that contains one row for each email in `texts` and one column for each word in `words`. If the $j$-th word in `words` is present at least once in the $i$-th email in `texts`, the output array should have a value of 1 at the position $(i, j)$. Otherwise, if the $j$-th word is not present in the $i$-th email, the value at $(i, j)$ should be 0.\n",
    "\n",
    "In Project B2, we will be applying `words_in_texts` to some large datasets, so implementing some form of vectorization (for example, using `NumPy` arrays, `Series.str` functions, etc.) is highly recommended. **You are allowed to use only *one* list comprehension or for loop**, and you should look into how you could combine that with the vectorized functions discussed above. **Do not use a double for loop, or you will run into issues later on in Project B2.**\n",
    "\n",
    "For example:\n",
    "```\n",
    ">>> words_in_texts(['hello', 'bye', 'world'], \n",
    "                   pd.Series(['hello', 'hello worldhello']))\n",
    "\n",
    "array([[1, 0, 0],\n",
    "       [1, 0, 1]])\n",
    "```\n",
    "\n",
    "Importantly, we **do not** calculate the *number of occurrences* of each word; only if the word is present at least *once*. Take a moment to work through the example on your own if need be â€”â€” understanding what the function does is a critical first step in implementing it.\n",
    "\n",
    "*The provided tests make sure that your function works correctly so that you can use it for future questions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.337281Z",
     "start_time": "2019-04-03T20:17:42.320567Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def words_in_texts(words, texts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        words (list): Words to find.\n",
    "        texts (Series): Strings to search in.\n",
    "    \n",
    "    Returns:\n",
    "        A 2D NumPy array of 0s and 1s with shape (n, d) where \n",
    "        n is the number of texts, and d is the number of words.\n",
    "    \"\"\"\n",
    "    indicator_array = np.array([texts.str.contains(word, regex=False).astype(int) for word in words]).T\n",
    "    return indicator_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to see what your function outputs. Compare the results to the example provided above.\n",
    "words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello worldhello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! ðŸŽ‰</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "eda",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 3: EDA\n",
    "\n",
    "We need to identify some features that allow us to distinguish spam emails from ham emails. One idea is to compare the distribution of a single feature in spam emails to the distribution of the same feature in ham emails. Suppose the feature is a binary indicator, such as whether a particular word occurs in the text. In that case, this compares the proportion of spam emails with the word to the proportion of ham emails with the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The following plot (created using `sns.barplot`) compares the proportion of emails in each class containing a particular set of words. The bars colored by email class were generated by setting the `hue` parameter of `sns.barplot` to a column containing the class (spam or ham) of each data point. An example of how this class column was created is shown below:\n",
    "\n",
    "![training conditional proportions](images/training_conditional_proportions.png)\n",
    "\n",
    "You can use `DataFrame`'s `.melt` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html)) method to \"unpivot\" a `DataFrame`. See the following code cell for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:42.428419Z",
     "start_time": "2019-04-03T20:17:42.386697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Our original `DataFrame` has a `type` column and some columns corresponding to words. You can think of each row as a sentence, and the value of 1 or 0 indicates the number of occurrences of the word in this sentence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_1  word_2  type\n",
       "0       1       0  spam\n",
       "1       0       1   ham\n",
       "2       1       0   ham\n",
       "3       0       1   ham"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> `melt` will turn columns into entries in a variable column. Notice how `word_1` and `word_2` become entries in `variable`; their values are stored in the `value` column."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>word_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>word_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>word_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>word_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>word_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>word_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>word_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>word_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type variable  value\n",
       "0  spam   word_1      1\n",
       "1   ham   word_1      0\n",
       "2   ham   word_1      1\n",
       "3   ham   word_1      0\n",
       "4  spam   word_2      0\n",
       "5   ham   word_2      1\n",
       "6   ham   word_2      0\n",
       "7   ham   word_2      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "df = pd.DataFrame({\n",
    "    'word_1': [1, 0, 1, 0],\n",
    "    'word_2': [0, 1, 0, 1],\n",
    "    'type': ['spam', 'ham', 'ham', 'ham']\n",
    "})\n",
    "display(Markdown(\"> Our original `DataFrame` has a `type` column and some columns corresponding to words. You can think of each row as a sentence, and the value of 1 or 0 indicates the number of occurrences of the word in this sentence.\"))\n",
    "display(df);\n",
    "display(Markdown(\"> `melt` will turn columns into entries in a variable column. Notice how `word_1` and `word_2` become entries in `variable`; their values are stored in the `value` column.\"))\n",
    "display(df.melt(\"type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Create the bar chart above by comparing the proportion of spam and ham emails containing specific words. **Choose a set of 6 words other than those shown in the example.** These words should have different proportions for the two classes (i.e., noticeably different bar heights across spam and ham). Make sure only to consider emails from `train`. Your `words_in_texts` function from the previous part will be useful here. \n",
    "\n",
    "**Hint:** This is a pretty challenging question. The suggested approach is to first look at the example bar plot and make sure you can interpret what is being plotted - what does a bar represent? What does the height mean? \n",
    "\n",
    "Next, see how to make this plot with `sns.barplot`. Take a look at the [documentation](https://seaborn.pydata.org/generated/seaborn.barplot.html) and determine what the inputs should be. A possible data input is given below:\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>type</th>      <th>variable</th>      <th>value</th>    </tr>  </thead>  <tbody>    \n",
    "<tr>      <th>0</th>      <td>Ham</td>      <td>word_1</td>      <td>0.021269</td>    </tr>    \n",
    "<tr>      <th>1</th>      <td>Ham</td>      <td>word_2</td>      <td>0.101519</td>    </tr>    \n",
    "<tr>      <th>2</th>      <td>Spam</td>      <td>word_3</td>      <td>0.059160</td>    </tr>    \n",
    "<tr>      <th>3</th>      <td>Spam</td>      <td>word_2</td>      <td>0.017694</td>    </tr>    \n",
    "<tr>      <th>4</th>      <td>Ham</td>      <td>word_4</td>      <td>0.013226</td>    </tr>     \n",
    "<tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    \n",
    "</tbody></table>\n",
    "\n",
    "Finally, you will need to chain some `pandas` functions together. Try to add one function at a time and see how that affects the `DataFrame`. It may help to use a new cell or print out the `DataFrame` for debugging purposes as you work towards achieving the desired format above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Create your bar chart in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:43.145246Z",
     "start_time": "2019-04-03T20:17:42.430406Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "q3a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.reset_index(drop=True) # We must do this in order to preserve the ordering of emails to labels for words_in_texts.\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "words = ['free', 'click', 'winner', 'offer', 'dollar', 'urgent']\n",
    "word_array = words_in_texts(words, train['email'])\n",
    "\n",
    "word_df = pd.DataFrame(word_array, columns = words)\n",
    "word_df['type'] = train['spam'].map({0: 'Ham', 1: 'Spam'})\n",
    "\n",
    "melted = word_df.melt(id_vars= 'type')\n",
    "grouped = melted.groupby(['type', 'variable'])['value'].mean().reset_index()\n",
    "\n",
    "sns.barplot(data=grouped, x='variable', y='value', hue='type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "When the feature is binary, it makes sense to compare its proportions across classes (as in the previous question). Otherwise, if the feature can take on numeric values, we can compare the distributions of these values for different classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "classification",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 4: Basic Classification\n",
    "\n",
    "Notice that the output of `words_in_texts(words, train['email'])` is a numeric matrix containing features for each email. This means we can use it directly to train a classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "We've given you 5 words that might be useful as features to distinguish spam/ham emails. Use these words and the `train` `DataFrame` to create two `NumPy` arrays: `X_train` and `Y_train`. `X_train` should be a 2D array of 0s and 1s created using your `words_in_texts` function on all the emails in the training set. `Y_train` should be a vector of the correct labels for each email in the training set.\n",
    "\n",
    "*The provided tests check that the dimensions of your design matrix ($\\mathbb{X}$) are correct and that your features and labels are binary (i.e., consist only of 0s and 1s). It does not check that your function is correct; that was verified in Question 2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:43.726012Z",
     "start_time": "2019-04-03T20:17:43.498088Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q4-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0]]),\n",
       " array([0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "\n",
    "X_train = words_in_texts(['drug', 'bank', 'prescription', 'memo', 'private'], train['email'])\n",
    "Y_train = train['spam'].values\n",
    "\n",
    "X_train[:5], Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed! ðŸŒˆ</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "## Question 5\n",
    "\n",
    "Now that we have matrices, we can build a model with `sklearn`! Using the [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier, train a logistic regression model using `X_train` and `Y_train`. Then, output the model's training accuracy below. You should get an accuracy of around $0.76$.\n",
    "\n",
    "*The provided tests check that you initialized your logistic regression model correctly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:17:44.593918Z",
     "start_time": "2019-04-03T20:17:43.783872Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q5-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7576201251164648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "my_model = LogisticRegression()\n",
    "my_model.fit(X_train, Y_train)\n",
    "\n",
    "training_accuracy = my_model.score(X_train, Y_train)\n",
    "print(\"Training Accuracy: \", training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.allclose(my_model.coef_, np.array([[ 0.3876794 ,  1.41303343,  2.04437707, -0.53676679,  0.92334944]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed! ðŸŒˆ</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 5: Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem too shabby! But the classifier you made above isn't as good as the accuracy would make you believe. First, we are evaluating the accuracy of the model on the training set, which may be a misleading measure. Accuracy on the training set doesn't always translate to accuracy in the real world (on the test set). In future parts of this analysis, we will make use of the data we held out for model validation and comparison.\n",
    "\n",
    "Presumably, our classifier will be used for **filtering**, or preventing messages labeled `spam` from reaching someone's inbox. There are two kinds of errors we can make:\n",
    "- **False positive (FP)**: A ham email gets flagged as spam and filtered out of the inbox.\n",
    "- **False negative (FN)**: A spam email gets mislabeled as ham and ends up in the inbox.\n",
    "\n",
    "To be clear, we label spam emails as 1 and ham emails as 0. These definitions depend both on the true labels and the predicted labels. False positives and false negatives may be of differing importance, leading us to consider more ways of evaluating a classifier in addition to overall accuracy:\n",
    "\n",
    "**Precision**: Measures the proportion of emails flagged as spam that are actually spam. Mathematically, $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$.\n",
    "\n",
    "**Recall**: Measures the proportion  of spam emails that were correctly flagged as spam. Mathematically, $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$.\n",
    "\n",
    "**False positive rate**: Measures the proportion  of ham emails that were incorrectly flagged as spam. Mathematically, $\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$.\n",
    "\n",
    "One quick mnemonic to remember the formulas is that **P**recision involves T**P** and F**P**, Recall does not. In the final, the reference sheet will also contain the formulas shown above, but you should be able to interpret what they mean and their importance depending on the context.\n",
    "\n",
    "The below graphic (modified slightly from [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)) may help you understand precision and recall visually:<br />\n",
    "<center>\n",
    "<img alt=\"precision_recall\" src=\"images/precision_recall.png\" width=\"600px\" />\n",
    "</center>\n",
    "\n",
    "Note that a True Positive (TP) is a spam email that is classified as spam, and a True Negative (TN) is a ham email that is classified as ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6a\n",
    "\n",
    "Suppose we have a hypothetical classifier called the â€œzero predictor.â€ For any inputted email, the zero predictor *always* predicts 0 (it never makes a prediction of 1 for any email). How many false positives and false negatives would this classifier have if it were evaluated on the training set and its results were compared to `Y_train`? Assign `zero_predictor_fp` to the number of false positives and `zero_predictor_fn` to the number of false negatives for the hypothetical zero predictor on the training data.\n",
    "\n",
    "*The public tests only check that you have assigned appropriate types of values to each response variable but do not check that your answers are correct. That is, we only check that the number of false positives and false negatives should be greater than or equal to 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:20:13.853633Z",
     "start_time": "2019-04-03T20:20:13.825724Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "q6a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1918)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_predictor_fp = 0\n",
    "zero_predictor_fn = sum(Y_train)\n",
    "zero_predictor_fp, zero_predictor_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6a</pre></strong> passed! ðŸ€</p>"
      ],
      "text/plain": [
       "q6a results: All test cases passed!"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6b\n",
    "\n",
    "What is the accuracy and recall of the zero predictor on the training data? Do not use any `sklearn` functions to compute these performance metrics.\n",
    "\n",
    "*The public tests only check that you have assigned appropriate types of values to each response variable but do not check that your answers are correct. That is, we only check that proportions or percentages (like precision, recall, accuracy) lie in the interval [0, 1].*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:23:21.553134Z",
     "start_time": "2019-04-03T20:23:21.548219Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7447091707706642, 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_predictor_acc = (len(Y_train) - sum(Y_train)) / len(Y_train)\n",
    "zero_predictor_recall = 0\n",
    "zero_predictor_acc, zero_predictor_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6b</pre></strong> passed! ðŸ™Œ</p>"
      ],
      "text/plain": [
       "q6b results: All test cases passed!"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6c\n",
    "\n",
    "Explain your results in `q6a` and `q6b`. How did you know what to assign to `zero_predictor_fp`, `zero_predictor_fn`, `zero_predictor_acc`, and `zero_predictor_recall`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "zero_predictor_fp - happens when we incorrectly predict ham. Since the zero predictor never predicts anything as spam, it can't make false positives, so fp=0.\n",
    "\n",
    "zero_predictor_fn - happens when a spam email is mislabeled as ham. Since every ham is labeled 0, the number of false negatives is the sum of all the training data.\n",
    "\n",
    "zero_predictor_acc - accurace is the number of correct predictions / total predictions made. This translates to the number of ham emails or (len(Y_train) - sum(Y_train)) / the total number of emails or len(Y_train)\n",
    "\n",
    "zero_predictor_recall - recall is equal to the number of true positives divided by the total total number of spam emails. Since the predictor never identifies anything as spam, this number is 0 / total emails = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6d\n",
    "\n",
    "Compute the precision, recall, and false positive rate of the `LogisticRegression` classifier `my_model` from Question 5. Do **not** use any `sklearn` functions to compute performance metrics; the only `sklearn` method you may use here is `.predict` to generate model predictions using `my_model` and `X_train`.\n",
    "\n",
    "*The public tests only check that you have assigned appropriate types of values to each response variable but do not check that your answers are correct. That is, we only check that proportions or percentages (like precision, recall, false positive rate) lie in the interval [0, 1].*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T20:37:54.875265Z",
     "start_time": "2019-04-03T20:37:54.720667Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=219, TN=5473, FP=122, FN=1699\n",
      "logistic_predictor_precision=0.64, logistic_predictor_recall=0.64, logistic_predictor_fpr=0.36\n"
     ]
    }
   ],
   "source": [
    "Y_train_hat = my_model.predict(X_train)\n",
    "\n",
    "TP = np.sum((Y_train_hat == 1) & (Y_train == 1))\n",
    "TN = np.sum((Y_train_hat == 0) & (Y_train == 0))\n",
    "FP = np.sum((Y_train_hat == 1) & (Y_train == 0))\n",
    "FN = np.sum((Y_train_hat == 0) & (Y_train == 1))\n",
    "logistic_predictor_precision = TP / (TP + FP)\n",
    "logistic_predictor_recall = TP / (TP + FP)\n",
    "logistic_predictor_fpr = FP / (TP + FP)\n",
    "\n",
    "print(f\"{TP=}, {TN=}, {FP=}, {FN=}\")\n",
    "print(f\"{logistic_predictor_precision=:.2f}, {logistic_predictor_recall=:.2f}, {logistic_predictor_fpr=:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6d</pre></strong> passed! ðŸ™Œ</p>"
      ],
      "text/plain": [
       "q6d results: All test cases passed!"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6e\n",
    "\n",
    "Is the number of false positives produced by the logistic regression classifier `my_model` strictly less than the number of false negatives produced? Assign to `q6e` an expression that evaluates to give your answer (`True` or `False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q6e = FP < FN\n",
    "q6e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6e</pre></strong> passed! âœ¨</p>"
      ],
      "text/plain": [
       "q6e results: All test cases passed!"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6f\n",
    "\n",
    "How does the accuracy of the logistic regression classifier `my_model` compare to the accuracy of the zero predictor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "The accuracy of my_model tries to distinguish between spam and ham based on word presence. While my_model makes false positives and false negatives, it usually establishes a better balance between precision and recall with higher accuracy. While the zero predictor always predicts 0, so its accuracy is just the proportion of ham emails in the training data. The zero predictor may have decent accuracy when most emails are ham, but it completely fails in identifying spam, meaning it has zero recall and several false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6g\n",
    "\n",
    "Given the word features provided in Question 4, discuss why the logistic regression classifier `my_model` may be performing poorly. \n",
    "\n",
    "**Hint:** Think about how prevalent these words are in the email set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "my_model was likely performing poorly with the features provided in question 4 because those words only apply in such a small number of emails, that the rows in its feature matrix are just 0 which makes it difficult to find patterns. Also, some of the words like 'private' and 'memo' are pretty likely to appear in both spam and ham emails, as they are just common email topics in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6h\n",
    "\n",
    "Would you prefer to use the logistic regression classifier `my_model` or the zero predictor classifier for a spam filter? Why? Describe your reasoning and relate it to at least one of the evaluation metrics you have computed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "I would rather use the logistic regregression classifier. Even though the zero predictor can have decent accuracy, its complete inability to predict spam makes it useless as a spam filter. my_model will detect some spam meaning it can succesfully stop people from recieving spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Pishi, Nori, and Socks congratulate you for finishing Project B1!\n",
    "<div align=\"middle\">\n",
    "    <table style=\"width:100%\">\n",
    "      <tr align=\"center\">\n",
    "        <td><img src=\"images/pishi.jpg\" align=\"middle\" width=\"600vw\" />\n",
    "        <td><video controls src = \"images/nori_snow.MP4\" width = \"250\">animation</video>\n",
    "        </td>\n",
    "        <td><img src=\"images/socks\" align=\"middle\" width=\"800vw\" />\n",
    "      </tr>\n",
    "    </table>\n",
    "  </div>\n",
    "\n",
    "What's next? In Project B2, you will focus on building a spam/ham email classifier with logistic regression. You will be well-prepared to build such a model: you have considered what is in this data set, what it can be used for, and engineered some features that should be useful for prediction.\n",
    "\n",
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/Rur7zXwdyeWnWEX67). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the Project B1 Coding assignment on Gradescope, Gradescope will automatically submit a PDF file with your written answers to the Project B1 Written assignment. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide.\n",
    "\n",
    "If there are issues with automatically generating the PDF, please check this [section](https://ds100.org/debugging-guide/jupyter_datahub/jupyter_datahub.html#i-cant-export-my-assignment-as-a-pdf-due-to-a-latexfailed-error) of the Debugging Guide for alternative options.\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that everything was generated and submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running your submission against local test cases...\n",
      "\n",
      "\n",
      "Your submission received the following results when run against available test cases:\n",
      "\n",
      "    q2 results: All test cases passed!\n",
      "\n",
      "    q4 results: All test cases passed!\n",
      "\n",
      "    q5 results: All test cases passed!\n",
      "\n",
      "    q6a results: All test cases passed!\n",
      "\n",
      "    q6b results: All test cases passed!\n",
      "\n",
      "    q6d results: All test cases passed!\n",
      "\n",
      "    q6e results: All test cases passed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <p>\n",
       "                        Your submission has been exported. Click\n",
       "                        <a href=\"projB1_2025_04_25T05_53_54_660034.zip\" download=\"projB1_2025_04_25T05_53_54_660034.zip\" target=\"_blank\">here</a> to download\n",
       "                        the zip file.\n",
       "                    </p>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q2": {
     "name": "q2",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(type(words_in_texts([], pd.Series([]))) == np.ndarray)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello worldhello'])).shape == (2, 3))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.allclose(words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello worldhello'])), np.array([[1, 0, 0], [1, 0, 1]])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.allclose(words_in_texts(['a', 'b', 'c', 'd', 'e', 'f', 'g'], pd.Series(['a b c d ef g', 'a', 'b', 'c', 'd e f g', 'h', 'a h'])), np.array([[1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0]])))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(type(Y_train) == np.ndarray)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(X_train.shape == (7513, 5))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.array_equal(np.unique(X_train), np.array([0, 1])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.array_equal(np.unique(Y_train), np.array([0, 1])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.all(X_train[:5] == np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 1, 0]])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.all(Y_train[:5] == np.array([0, 0, 0, 0, 0])))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(training_accuracy > 0.75)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert zero_predictor_fn >= 0\n>>> assert zero_predictor_fp >= 0\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 0 <= zero_predictor_acc <= 1\n>>> assert 0 <= zero_predictor_recall <= 1\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6d": {
     "name": "q6d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 0 <= logistic_predictor_precision <= 1\n>>> assert 0 <= logistic_predictor_recall <= 1\n>>> assert 0 <= logistic_predictor_fpr <= 1\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6e": {
     "name": "q6e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(q6e in [True, False])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
